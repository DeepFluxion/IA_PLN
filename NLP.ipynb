{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aula Prática de PROCESSAMENTO DE LINGUAGEM NATURAL\n",
    "\n",
    "![Biblioteca Para Processamento de Linguagem Natural](python_nltk.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importação da Biblioteca\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Textos para análise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# texto em inglês\n",
    "text = 'Backgammon is one of the oldest known board games. Its history can be traced back nearly 5,000 years to archeological discoveries in the Middle East. It is a two player game where each player has fifteen checkers which move between twenty-four points according to the roll of two dice.'\n",
    "# texto em português\n",
    "texto = 'Gamão é um dos mais antigos jogos de tabuleiro conhecidos. Sua história remonta quase 5.000 anos às descobertas arqueológicas no Oriente Médio. É um jogo para dois jogadores, onde cada jogador tem quinze damas que se movem entre vinte e quatro pontos de acordo com o lançamento de dois dados.'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenização de Sentenças"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Backgammon is one of the oldest known board games.\n",
      "\n",
      "Its history can be traced back nearly 5,000 years to archeological discoveries in the Middle East.\n",
      "\n",
      "It is a two player game where each player has fifteen checkers which move between twenty-four points according to the roll of two dice.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## Tokenização das Sentenças em Inglês\n",
    "sentences = nltk.sent_tokenize(text)\n",
    "\n",
    "for sent in sentences:\n",
    "    print(sent)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gamão é um dos mais antigos jogos de tabuleiro conhecidos.\n",
      "\n",
      "Sua história remonta quase 5.000 anos às descobertas arqueológicas no Oriente Médio.\n",
      "\n",
      "É um jogo para dois jogadores, onde cada jogador tem quinze damas que se movem entre vinte e quatro pontos de acordo com o lançamento de dois dados.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## Tokenização das Sentenças em Português\n",
    "sentences = nltk.sent_tokenize(texto)\n",
    "\n",
    "for sent in sentences:\n",
    "    print(sent)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenização de Palavras por Sentenças"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Backgammon', 'is', 'one', 'of', 'the', 'oldest', 'known', 'board', 'games', '.']\n",
      "\n",
      "['Its', 'history', 'can', 'be', 'traced', 'back', 'nearly', '5,000', 'years', 'to', 'archeological', 'discoveries', 'in', 'the', 'Middle', 'East', '.']\n",
      "\n",
      "['It', 'is', 'a', 'two', 'player', 'game', 'where', 'each', 'player', 'has', 'fifteen', 'checkers', 'which', 'move', 'between', 'twenty-four', 'points', 'according', 'to', 'the', 'roll', 'of', 'two', 'dice', '.']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## Tokenização das Palavras por Sentença em Inglês\n",
    "## Passo 1 Tokenização das Sentenças em Inglês\n",
    "sentences = nltk.sent_tokenize(text)\n",
    "\n",
    "## Passo 2 Tokenização das Palavras por Sentença em Inglês\n",
    "for sent in sentences:\n",
    "    words = nltk.word_tokenize(sent)\n",
    "    print(words)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Gamão', 'é', 'um', 'dos', 'mais', 'antigos', 'jogos', 'de', 'tabuleiro', 'conhecidos', '.']\n",
      "\n",
      "['Sua', 'história', 'remonta', 'quase', '5.000', 'anos', 'às', 'descobertas', 'arqueológicas', 'no', 'Oriente', 'Médio', '.']\n",
      "\n",
      "['É', 'um', 'jogo', 'para', 'dois', 'jogadores', ',', 'onde', 'cada', 'jogador', 'tem', 'quinze', 'damas', 'que', 'se', 'movem', 'entre', 'vinte', 'e', 'quatro', 'pontos', 'de', 'acordo', 'com', 'o', 'lançamento', 'de', 'dois', 'dados', '.']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## Tokenização das Palavras por Sentença em Português\n",
    "## Passo 1 Tokenização das Sentenças em Português\n",
    "sentences = nltk.sent_tokenize(texto)\n",
    "\n",
    "## Passo 2 Tokenização das Palavras por Sentença em Português\n",
    "for sent in sentences:\n",
    "    words = nltk.word_tokenize(sent)\n",
    "    print(words)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stemming e Lematização"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\domin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\domin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Importação da Biblioteca dos Stemmer e Lemmatizer\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________\n",
      "Backgammon => backgammon\n",
      "is => is\n",
      "one => one\n",
      "of => of\n",
      "the => the\n",
      "oldest => oldest\n",
      "known => known\n",
      "board => board\n",
      "games => game\n",
      ". => .\n",
      "_________________________\n",
      "Its => it\n",
      "history => histori\n",
      "can => can\n",
      "be => be\n",
      "traced => trace\n",
      "back => back\n",
      "nearly => nearli\n",
      "5,000 => 5,000\n",
      "years => year\n",
      "to => to\n",
      "archeological => archeolog\n",
      "discoveries => discoveri\n",
      "in => in\n",
      "the => the\n",
      "Middle => middl\n",
      "East => east\n",
      ". => .\n",
      "_________________________\n",
      "It => It\n",
      "is => is\n",
      "a => a\n",
      "two => two\n",
      "player => player\n",
      "game => game\n",
      "where => where\n",
      "each => each\n",
      "player => player\n",
      "has => ha\n",
      "fifteen => fifteen\n",
      "checkers => checker\n",
      "which => which\n",
      "move => move\n",
      "between => between\n",
      "twenty-four => twenty-four\n",
      "points => point\n",
      "according => accord\n",
      "to => to\n",
      "the => the\n",
      "roll => roll\n",
      "of => of\n",
      "two => two\n",
      "dice => dice\n",
      ". => .\n"
     ]
    }
   ],
   "source": [
    "stemmer = PorterStemmer()\n",
    "\n",
    "## Passo 1 Tokenização das Sentenças em Português\n",
    "sentences = nltk.sent_tokenize(text)\n",
    "## Passo 2 Stemming das Palavras Tokenizadas por Sentença em Português\n",
    "text_stemmed = []\n",
    "for sent in sentences:\n",
    "    words = nltk.word_tokenize(sent)\n",
    "    print('_________________________')\n",
    "    for word in words:\n",
    "        print(word +' => '+stemmer.stem(word))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### POS Tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________\n",
      "('Backgammon', 'NNP')\n",
      "('is', 'VBZ')\n",
      "('one', 'CD')\n",
      "('of', 'IN')\n",
      "('the', 'DT')\n",
      "('oldest', 'JJS')\n",
      "('known', 'VBN')\n",
      "('board', 'NN')\n",
      "('games', 'NNS')\n",
      "('.', '.')\n",
      "_________________________\n",
      "('Its', 'PRP$')\n",
      "('history', 'NN')\n",
      "('can', 'MD')\n",
      "('be', 'VB')\n",
      "('traced', 'VBN')\n",
      "('back', 'RB')\n",
      "('nearly', 'RB')\n",
      "('5,000', 'CD')\n",
      "('years', 'NNS')\n",
      "('to', 'TO')\n",
      "('archeological', 'JJ')\n",
      "('discoveries', 'NNS')\n",
      "('in', 'IN')\n",
      "('the', 'DT')\n",
      "('Middle', 'NNP')\n",
      "('East', 'NNP')\n",
      "('.', '.')\n",
      "_________________________\n",
      "('It', 'PRP')\n",
      "('is', 'VBZ')\n",
      "('a', 'DT')\n",
      "('two', 'CD')\n",
      "('player', 'NN')\n",
      "('game', 'NN')\n",
      "('where', 'WRB')\n",
      "('each', 'DT')\n",
      "('player', 'NN')\n",
      "('has', 'VBZ')\n",
      "('fifteen', 'VBN')\n",
      "('checkers', 'NNS')\n",
      "('which', 'WDT')\n",
      "('move', 'VBP')\n",
      "('between', 'IN')\n",
      "('twenty-four', 'NN')\n",
      "('points', 'NNS')\n",
      "('according', 'VBG')\n",
      "('to', 'TO')\n",
      "('the', 'DT')\n",
      "('roll', 'NN')\n",
      "('of', 'IN')\n",
      "('two', 'CD')\n",
      "('dice', 'NNS')\n",
      "('.', '.')\n"
     ]
    }
   ],
   "source": [
    "## Passo 1 Tokenização das Sentenças em Inglês\n",
    "sentences = nltk.sent_tokenize(text)\n",
    "## Passo 2 Tagging das Palavras Tokenizadas por Sentença em Inglês\n",
    "\n",
    "for sent in sentences:\n",
    "    print('_________________________')\n",
    "    for tag in nltk.pos_tag(nltk.word_tokenize(sent)):\n",
    "        print(tag)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## POS Tagging e Lematização"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________\n",
      "is => be\n",
      "oldest => old\n",
      "known => know\n",
      "games => game\n",
      "games => game\n",
      "_________________________\n",
      "traced => trace\n",
      "years => year\n",
      "discoveries => discovery\n",
      "_________________________\n",
      "is => be\n",
      "has => ha\n",
      "has => have\n",
      "checkers => checker\n",
      "checkers => checker\n",
      "points => point\n",
      "points => point\n",
      "according => accord\n"
     ]
    }
   ],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "## Passo 1 Tokenização das Sentenças em Português\n",
    "sentences = nltk.sent_tokenize(text)\n",
    "## Passo 2 Stemming das Palavras Tokenizadas por Sentença em Português\n",
    "tag_list = [wordnet.ADJ,wordnet.NOUN,wordnet.VERB,wordnet.ADV]\n",
    "for sent in sentences:\n",
    "    words = nltk.word_tokenize(sent)\n",
    "    print('_________________________')\n",
    "    for word in words:\n",
    "            for tag in tag_list:\n",
    "                if word !=lemmatizer.lemmatize(word,pos = tag):\n",
    "                    print(word +' => '+lemmatizer.lemmatize(word,pos = tag))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remoção de Stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
     ]
    }
   ],
   "source": [
    "## Remoção de Stopword em Ingles\n",
    "from nltk.corpus import stopwords\n",
    "print(stopwords.words(\"english\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['de', 'a', 'o', 'que', 'e', 'do', 'da', 'em', 'um', 'para', 'com', 'não', 'uma', 'os', 'no', 'se', 'na', 'por', 'mais', 'as', 'dos', 'como', 'mas', 'ao', 'ele', 'das', 'à', 'seu', 'sua', 'ou', 'quando', 'muito', 'nos', 'já', 'eu', 'também', 'só', 'pelo', 'pela', 'até', 'isso', 'ela', 'entre', 'depois', 'sem', 'mesmo', 'aos', 'seus', 'quem', 'nas', 'me', 'esse', 'eles', 'você', 'essa', 'num', 'nem', 'suas', 'meu', 'às', 'minha', 'numa', 'pelos', 'elas', 'qual', 'nós', 'lhe', 'deles', 'essas', 'esses', 'pelas', 'este', 'dele', 'tu', 'te', 'vocês', 'vos', 'lhes', 'meus', 'minhas', 'teu', 'tua', 'teus', 'tuas', 'nosso', 'nossa', 'nossos', 'nossas', 'dela', 'delas', 'esta', 'estes', 'estas', 'aquele', 'aquela', 'aqueles', 'aquelas', 'isto', 'aquilo', 'estou', 'está', 'estamos', 'estão', 'estive', 'esteve', 'estivemos', 'estiveram', 'estava', 'estávamos', 'estavam', 'estivera', 'estivéramos', 'esteja', 'estejamos', 'estejam', 'estivesse', 'estivéssemos', 'estivessem', 'estiver', 'estivermos', 'estiverem', 'hei', 'há', 'havemos', 'hão', 'houve', 'houvemos', 'houveram', 'houvera', 'houvéramos', 'haja', 'hajamos', 'hajam', 'houvesse', 'houvéssemos', 'houvessem', 'houver', 'houvermos', 'houverem', 'houverei', 'houverá', 'houveremos', 'houverão', 'houveria', 'houveríamos', 'houveriam', 'sou', 'somos', 'são', 'era', 'éramos', 'eram', 'fui', 'foi', 'fomos', 'foram', 'fora', 'fôramos', 'seja', 'sejamos', 'sejam', 'fosse', 'fôssemos', 'fossem', 'for', 'formos', 'forem', 'serei', 'será', 'seremos', 'serão', 'seria', 'seríamos', 'seriam', 'tenho', 'tem', 'temos', 'tém', 'tinha', 'tínhamos', 'tinham', 'tive', 'teve', 'tivemos', 'tiveram', 'tivera', 'tivéramos', 'tenha', 'tenhamos', 'tenham', 'tivesse', 'tivéssemos', 'tivessem', 'tiver', 'tivermos', 'tiverem', 'terei', 'terá', 'teremos', 'terão', 'teria', 'teríamos', 'teriam']\n"
     ]
    }
   ],
   "source": [
    "## Remoção de Stopword em Português\n",
    "from nltk.corpus import stopwords\n",
    "print(stopwords.words(\"portuguese\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Backgammon is one of the oldest known board games.\n",
      "['Backgammon', 'is', 'one', 'of', 'the', 'oldest', 'known', 'board', 'games', '.']\n",
      "['Backgammon', 'one', 'oldest', 'known', 'board', 'games', '.']\n",
      "Its history can be traced back nearly 5,000 years to archeological discoveries in the Middle East.\n",
      "['Its', 'history', 'can', 'be', 'traced', 'back', 'nearly', '5,000', 'years', 'to', 'archeological', 'discoveries', 'in', 'the', 'Middle', 'East', '.']\n",
      "['Its', 'history', 'traced', 'back', 'nearly', '5,000', 'years', 'archeological', 'discoveries', 'Middle', 'East', '.']\n",
      "It is a two player game where each player has fifteen checkers which move between twenty-four points according to the roll of two dice.\n",
      "['It', 'is', 'a', 'two', 'player', 'game', 'where', 'each', 'player', 'has', 'fifteen', 'checkers', 'which', 'move', 'between', 'twenty-four', 'points', 'according', 'to', 'the', 'roll', 'of', 'two', 'dice', '.']\n",
      "['It', 'two', 'player', 'game', 'player', 'fifteen', 'checkers', 'move', 'twenty-four', 'points', 'according', 'roll', 'two', 'dice', '.']\n"
     ]
    }
   ],
   "source": [
    "stop_words = set(stopwords.words(\"english\"))\n",
    "\n",
    "for sentence in nltk.sent_tokenize(text):\n",
    "    print(sentence)\n",
    "    words = nltk.word_tokenize(sentence)\n",
    "    print(words)\n",
    "    without_stop_words = [word for word in words if not word in stop_words]\n",
    "    print(without_stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gamão é um dos mais antigos jogos de tabuleiro conhecidos.\n",
      "['Gamão', 'é', 'um', 'dos', 'mais', 'antigos', 'jogos', 'de', 'tabuleiro', 'conhecidos', '.']\n",
      "['Gamão', 'é', 'antigos', 'jogos', 'tabuleiro', 'conhecidos', '.']\n",
      "Sua história remonta quase 5.000 anos às descobertas arqueológicas no Oriente Médio.\n",
      "['Sua', 'história', 'remonta', 'quase', '5.000', 'anos', 'às', 'descobertas', 'arqueológicas', 'no', 'Oriente', 'Médio', '.']\n",
      "['Sua', 'história', 'remonta', 'quase', '5.000', 'anos', 'descobertas', 'arqueológicas', 'Oriente', 'Médio', '.']\n",
      "É um jogo para dois jogadores, onde cada jogador tem quinze damas que se movem entre vinte e quatro pontos de acordo com o lançamento de dois dados.\n",
      "['É', 'um', 'jogo', 'para', 'dois', 'jogadores', ',', 'onde', 'cada', 'jogador', 'tem', 'quinze', 'damas', 'que', 'se', 'movem', 'entre', 'vinte', 'e', 'quatro', 'pontos', 'de', 'acordo', 'com', 'o', 'lançamento', 'de', 'dois', 'dados', '.']\n",
      "['É', 'jogo', 'dois', 'jogadores', ',', 'onde', 'cada', 'jogador', 'quinze', 'damas', 'movem', 'vinte', 'quatro', 'pontos', 'acordo', 'lançamento', 'dois', 'dados', '.']\n"
     ]
    }
   ],
   "source": [
    "stop_words = set(stopwords.words(\"portuguese\"))\n",
    "\n",
    "for sentence in nltk.sent_tokenize(texto):\n",
    "    print(sentence)\n",
    "    words = nltk.word_tokenize(sentence)\n",
    "    print(words)\n",
    "    without_stop_words = [word for word in words if not word in stop_words]\n",
    "    print(without_stop_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bag of Words (BoW)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"I like this movie, it's funny.I hate this movie.\", 'This was awesome!', 'I like it.', 'Nice one.', 'I love it.']\n",
      "['Eu gosto deste filme, é engraçado.', 'Eu odeio esse filme.', 'Isso foi incrível!', 'Eu gosto.', 'Legal.', 'Eu amo.']\n"
     ]
    }
   ],
   "source": [
    "text2 = \"I like this movie, it's funny.I hate this movie. This was awesome! I like it. Nice one. I love it.\"\n",
    "texto2 = \"Eu gosto deste filme, é engraçado. Eu odeio esse filme. Isso foi incrível! Eu gosto. Legal. Eu amo.\"\n",
    "documents = nltk.sent_tokenize(text2)\n",
    "documentos = nltk.sent_tokenize(texto2)\n",
    "print(documents)\n",
    "print(documentos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>awesome</th>\n",
       "      <th>funny</th>\n",
       "      <th>hate</th>\n",
       "      <th>it</th>\n",
       "      <th>like</th>\n",
       "      <th>love</th>\n",
       "      <th>movie</th>\n",
       "      <th>nice</th>\n",
       "      <th>one</th>\n",
       "      <th>this</th>\n",
       "      <th>was</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   awesome  funny  hate  it  like  love  movie  nice  one  this  was\n",
       "0        0      1     1   1     1     0      2     0    0     2    0\n",
       "1        1      0     0   0     0     0      0     0    0     1    1\n",
       "2        0      0     0   1     1     0      0     0    0     0    0\n",
       "3        0      0     0   0     0     0      0     1    1     0    0\n",
       "4        0      0     0   1     0     1      0     0    0     0    0"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import as bibliotecas\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import pandas as pd\n",
    "\n",
    "# Passo 2. Crie o vcoabulario\n",
    "# O padrão de token remove os tokens de um único caractere. \n",
    "#É por isso que não temos os tokens \"I\" e \"s\" no modelo de saída usando a classe CountVectorizer mencionada acima.\n",
    "count_vectorizer = CountVectorizer()\n",
    "\n",
    "# Psso 3. Criar o modelo Bag-of-Words Model\n",
    "bag_of_words = count_vectorizer.fit_transform(documents)\n",
    "\n",
    "# Mostre o Modelo Bag-of-Words como um DataFrame Pandas (Tabela)\n",
    "feature_names = count_vectorizer.get_feature_names()\n",
    "pd.DataFrame(bag_of_words.toarray(), columns = feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>amo</th>\n",
       "      <th>deste</th>\n",
       "      <th>engraçado</th>\n",
       "      <th>esse</th>\n",
       "      <th>eu</th>\n",
       "      <th>filme</th>\n",
       "      <th>foi</th>\n",
       "      <th>gosto</th>\n",
       "      <th>incrível</th>\n",
       "      <th>isso</th>\n",
       "      <th>legal</th>\n",
       "      <th>odeio</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   amo  deste  engraçado  esse  eu  filme  foi  gosto  incrível  isso  legal  \\\n",
       "0    0      1          1     0   1      1    0      1         0     0      0   \n",
       "1    0      0          0     1   1      1    0      0         0     0      0   \n",
       "2    0      0          0     0   0      0    1      0         1     1      0   \n",
       "3    0      0          0     0   1      0    0      1         0     0      0   \n",
       "4    0      0          0     0   0      0    0      0         0     0      1   \n",
       "5    1      0          0     0   1      0    0      0         0     0      0   \n",
       "\n",
       "   odeio  \n",
       "0      0  \n",
       "1      1  \n",
       "2      0  \n",
       "3      0  \n",
       "4      0  \n",
       "5      0  "
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Passo 2. Crie o vcoabulario\n",
    "# O padrão de token remove os tokens de um único caractere. \n",
    "#É por isso que não temos os tokens \"I\" e \"s\" no modelo de saída usando a classe CountVectorizer mencionada acima.\n",
    "count_vectorizer = CountVectorizer()\n",
    "\n",
    "# Psso 3. Criar o modelo Bag-of-Words Model\n",
    "bag_of_words2 = count_vectorizer.fit_transform(documentos)\n",
    "\n",
    "# Mostre o Modelo Bag-of-Words como um DataFrame Pandas (Tabela)\n",
    "nomes_atributos = count_vectorizer.get_feature_names()\n",
    "pd.DataFrame(bag_of_words2.toarray(), columns = nomes_atributos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## N Gramas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "vamos primeiro construir uma função básica para construir N Gramas e deposi aplicá-la para apresentar os 2,3,4 Gramas dos testos exemplo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.util import ngrams\n",
    "\n",
    "def get_ngrams(text, n ):\n",
    "    n_grams = ngrams(word_tokenize(text), n)\n",
    "    return [ ' '.join(grams) for grams in n_grams]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bigramas em Inglês"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I like', 'like this', 'this movie', 'movie ,', ', it', \"it 's\", \"'s funny.I\", 'funny.I hate', 'hate this', 'this movie', 'movie .']\n",
      "['This was', 'was awesome', 'awesome !']\n",
      "['I like', 'like it', 'it .']\n",
      "['Nice one', 'one .']\n",
      "['I love', 'love it', 'it .']\n"
     ]
    }
   ],
   "source": [
    "for doc in documents:\n",
    "    print(get_ngrams(doc, 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trigramas em Inglês"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I like this', 'like this movie', 'this movie ,', 'movie , it', \", it 's\", \"it 's funny.I\", \"'s funny.I hate\", 'funny.I hate this', 'hate this movie', 'this movie .']\n",
      "['This was awesome', 'was awesome !']\n",
      "['I like it', 'like it .']\n",
      "['Nice one .']\n",
      "['I love it', 'love it .']\n"
     ]
    }
   ],
   "source": [
    "for doc in documents:\n",
    "    print(get_ngrams(doc, 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quadrigramas em Inglês"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I like this movie', 'like this movie ,', 'this movie , it', \"movie , it 's\", \", it 's funny.I\", \"it 's funny.I hate\", \"'s funny.I hate this\", 'funny.I hate this movie', 'hate this movie .']\n",
      "['This was awesome !']\n",
      "['I like it .']\n",
      "[]\n",
      "['I love it .']\n"
     ]
    }
   ],
   "source": [
    "for doc in documents:\n",
    "    print(get_ngrams(doc, 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bigramas em Português"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Eu gosto', 'gosto deste', 'deste filme', 'filme ,', ', é', 'é engraçado', 'engraçado .']\n",
      "['Eu odeio', 'odeio esse', 'esse filme', 'filme .']\n",
      "['Isso foi', 'foi incrível', 'incrível !']\n",
      "['Eu gosto', 'gosto .']\n",
      "['Legal .']\n",
      "['Eu amo', 'amo .']\n"
     ]
    }
   ],
   "source": [
    "for doc in documentos:\n",
    "    print(get_ngrams(doc, 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trigramas em Português"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I like this', 'like this movie', 'this movie ,', 'movie , it', \", it 's\", \"it 's funny.I\", \"'s funny.I hate\", 'funny.I hate this', 'hate this movie', 'this movie .']\n",
      "['This was awesome', 'was awesome !']\n",
      "['I like it', 'like it .']\n",
      "['Nice one .']\n",
      "['I love it', 'love it .']\n"
     ]
    }
   ],
   "source": [
    "for doc in documents:\n",
    "    print(get_ngrams(doc, 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quadrigramas em Português"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I like this movie', 'like this movie ,', 'this movie , it', \"movie , it 's\", \", it 's funny.I\", \"it 's funny.I hate\", \"'s funny.I hate this\", 'funny.I hate this movie', 'hate this movie .']\n",
      "['This was awesome !']\n",
      "['I like it .']\n",
      "[]\n",
      "['I love it .']\n"
     ]
    }
   ],
   "source": [
    "for doc in documents:\n",
    "    print(get_ngrams(doc, 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Matriz TDIDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "values = tfidf_vectorizer.fit_transform(documents)\n",
    "\n",
    "# Show the Model as a pandas DataFrame\n",
    "feature_names = tfidf_vectorizer.get_feature_names()\n",
    "TDIDF = pd.DataFrame(values.toarray(), columns = feature_names, index = documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>awesome</th>\n",
       "      <th>funny</th>\n",
       "      <th>hate</th>\n",
       "      <th>it</th>\n",
       "      <th>like</th>\n",
       "      <th>love</th>\n",
       "      <th>movie</th>\n",
       "      <th>nice</th>\n",
       "      <th>one</th>\n",
       "      <th>this</th>\n",
       "      <th>was</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>I like this movie, it's funny.I hate this movie.</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.321029</td>\n",
       "      <td>0.321029</td>\n",
       "      <td>0.214997</td>\n",
       "      <td>0.259005</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.642059</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.518009</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>This was awesome!</th>\n",
       "      <td>0.614189</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.495524</td>\n",
       "      <td>0.614189</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>I like it.</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.638711</td>\n",
       "      <td>0.769447</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Nice one.</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.707107</td>\n",
       "      <td>0.707107</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>I love it.</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.556451</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.830881</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   awesome     funny  \\\n",
       "I like this movie, it's funny.I hate this movie.  0.000000  0.321029   \n",
       "This was awesome!                                 0.614189  0.000000   \n",
       "I like it.                                        0.000000  0.000000   \n",
       "Nice one.                                         0.000000  0.000000   \n",
       "I love it.                                        0.000000  0.000000   \n",
       "\n",
       "                                                      hate        it  \\\n",
       "I like this movie, it's funny.I hate this movie.  0.321029  0.214997   \n",
       "This was awesome!                                 0.000000  0.000000   \n",
       "I like it.                                        0.000000  0.638711   \n",
       "Nice one.                                         0.000000  0.000000   \n",
       "I love it.                                        0.000000  0.556451   \n",
       "\n",
       "                                                      like      love  \\\n",
       "I like this movie, it's funny.I hate this movie.  0.259005  0.000000   \n",
       "This was awesome!                                 0.000000  0.000000   \n",
       "I like it.                                        0.769447  0.000000   \n",
       "Nice one.                                         0.000000  0.000000   \n",
       "I love it.                                        0.000000  0.830881   \n",
       "\n",
       "                                                     movie      nice  \\\n",
       "I like this movie, it's funny.I hate this movie.  0.642059  0.000000   \n",
       "This was awesome!                                 0.000000  0.000000   \n",
       "I like it.                                        0.000000  0.000000   \n",
       "Nice one.                                         0.000000  0.707107   \n",
       "I love it.                                        0.000000  0.000000   \n",
       "\n",
       "                                                       one      this       was  \n",
       "I like this movie, it's funny.I hate this movie.  0.000000  0.518009  0.000000  \n",
       "This was awesome!                                 0.000000  0.495524  0.614189  \n",
       "I like it.                                        0.000000  0.000000  0.000000  \n",
       "Nice one.                                         0.707107  0.000000  0.000000  \n",
       "I love it.                                        0.000000  0.000000  0.000000  "
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TDIDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "values = tfidf_vectorizer.fit_transform(documentos)\n",
    "\n",
    "# Show the Model as a pandas DataFrame\n",
    "feature_names = tfidf_vectorizer.get_feature_names()\n",
    "TDIDF = pd.DataFrame(values.toarray(), columns = feature_names, index = documentos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>amo</th>\n",
       "      <th>deste</th>\n",
       "      <th>engraçado</th>\n",
       "      <th>esse</th>\n",
       "      <th>eu</th>\n",
       "      <th>filme</th>\n",
       "      <th>foi</th>\n",
       "      <th>gosto</th>\n",
       "      <th>incrível</th>\n",
       "      <th>isso</th>\n",
       "      <th>legal</th>\n",
       "      <th>odeio</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Eu gosto deste filme, é engraçado.</th>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.5201</td>\n",
       "      <td>0.5201</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.308554</td>\n",
       "      <td>0.426489</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.426489</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Eu odeio esse filme.</th>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.575019</td>\n",
       "      <td>0.341135</td>\n",
       "      <td>0.471523</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.575019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Isso foi incrível!</th>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.57735</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.57735</td>\n",
       "      <td>0.57735</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Eu gosto.</th>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.586157</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.810198</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Legal.</th>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Eu amo.</th>\n",
       "      <td>0.86004</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.510227</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        amo   deste  engraçado      esse  \\\n",
       "Eu gosto deste filme, é engraçado.  0.00000  0.5201     0.5201  0.000000   \n",
       "Eu odeio esse filme.                0.00000  0.0000     0.0000  0.575019   \n",
       "Isso foi incrível!                  0.00000  0.0000     0.0000  0.000000   \n",
       "Eu gosto.                           0.00000  0.0000     0.0000  0.000000   \n",
       "Legal.                              0.00000  0.0000     0.0000  0.000000   \n",
       "Eu amo.                             0.86004  0.0000     0.0000  0.000000   \n",
       "\n",
       "                                          eu     filme      foi     gosto  \\\n",
       "Eu gosto deste filme, é engraçado.  0.308554  0.426489  0.00000  0.426489   \n",
       "Eu odeio esse filme.                0.341135  0.471523  0.00000  0.000000   \n",
       "Isso foi incrível!                  0.000000  0.000000  0.57735  0.000000   \n",
       "Eu gosto.                           0.586157  0.000000  0.00000  0.810198   \n",
       "Legal.                              0.000000  0.000000  0.00000  0.000000   \n",
       "Eu amo.                             0.510227  0.000000  0.00000  0.000000   \n",
       "\n",
       "                                    incrível     isso  legal     odeio  \n",
       "Eu gosto deste filme, é engraçado.   0.00000  0.00000    0.0  0.000000  \n",
       "Eu odeio esse filme.                 0.00000  0.00000    0.0  0.575019  \n",
       "Isso foi incrível!                   0.57735  0.57735    0.0  0.000000  \n",
       "Eu gosto.                            0.00000  0.00000    0.0  0.000000  \n",
       "Legal.                               0.00000  0.00000    1.0  0.000000  \n",
       "Eu amo.                              0.00000  0.00000    0.0  0.000000  "
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TDIDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
